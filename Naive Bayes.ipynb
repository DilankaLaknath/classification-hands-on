{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Naive Bayes Classifier</span> \n",
    "\n",
    "\n",
    "Naive Bayes is one of the most practical classification machine learning algorithms. \n",
    "\n",
    "* fast\n",
    "* good performance\n",
    "* simple yet very effective\n",
    "* robust to irrelative features\n",
    "\n",
    "So why is it called naive?\n",
    "\n",
    "Because it does not consider the dependency between features and assume all features are independent of each other which is not the case in reality. This is a naive assumption, hence the name.\n",
    "\n",
    "The accuracy is very good although this naive assumption. A famous example of NB usage is spam filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Example1\n",
    "\n",
    "We assume we have collected the below data for the past 5 days. Based on this data, can we predict if our subject will play in a setting like:\n",
    "\n",
    "    outlook  = overcast\n",
    "    temp     = hot\n",
    "    humidity = normal\n",
    "    windy    = no\n",
    "    \n",
    "<table border=1>\n",
    "  <tr>\n",
    "    <th>Outlook</th>\n",
    "    <th>Temperature</th>\n",
    "    <th>Humidity</th>\n",
    "    <th>Windy</th>\n",
    "    <th>Play</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>sunny</td>\n",
    "    <td>hot</td>\n",
    "    <td>normal</td>\n",
    "    <td>no</td>\n",
    "    <td>yes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>overcast</td>\n",
    "    <td>mild</td>\n",
    "    <td>normal</td>\n",
    "    <td>no</td>\n",
    "    <td>yes</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>rainy</td>\n",
    "    <td>cool</td>\n",
    "    <td>high</td>\n",
    "    <td>yes</td>\n",
    "    <td>no</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>sunny</td>\n",
    "    <td>mild</td>\n",
    "    <td>normal</td>\n",
    "    <td>yes</td>\n",
    "    <td>no</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>overcast</td>\n",
    "    <td>hot</td>\n",
    "    <td>high</td>\n",
    "    <td>no</td>\n",
    "    <td>no</td>\n",
    "  </tr>\n",
    "</table>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to find a representation for our data. We can construct a dictionary to convert stings into numbers and then save them in a dataframe. \n",
    "\n",
    "    outlook: sunny=0, overcast=1, rainy=2\n",
    "    temp: hot=0, mild=1, cool=2\n",
    "    humidity: normal=0, high=1\n",
    "    wind: no=0, yes=1\n",
    "    play: np=0, yes=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    " 'outlook': [0, 1, 2, 0, 1],\n",
    " 'temp' : [0, 1, 2, 1, 0],\n",
    " 'humid' : [0, 0, 1, 0, 1],\n",
    " 'wind' : [0, 0, 1, 1, 0],\n",
    " 'play' : [1, 1, 0, 0, 0,] \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use Bayes rule to construct a Naive Bayes classifier. We can write:\n",
    "\n",
    "$$Pr\\left(p|o,t,h,w\\right)\\propto Pr\\left(p\\right)Pr(o|p)Pr(t|p)Pr(h|p)Pr(w|p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate $Pr(p)$ we use marginal probablity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 2), (2, 1)]\n",
      "{0: 0.4, 1: 0.4, 2: 0.2}\n",
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "def marginal_prob(df, column):\n",
    " '''\n",
    " Compute the marignal probability for values in a column\n",
    " '''\n",
    " # an array contain pairs of (value, count)\n",
    " vals_counts = [(val, (df[column] == val).sum()) for val in set(df[column])]\n",
    " print(vals_counts)\n",
    " total_count = sum([count for val, count in vals_counts])\n",
    " \n",
    " # an array contain pairs of (value, probability)\n",
    " vals_probs = [(val, count/total_count) for val, count in vals_counts]\n",
    " # a dictionary in which keys are val and values are the corresponding probabilities\n",
    " return dict(vals_probs)\n",
    "\n",
    "print(marginal_prob(df,'outlook'))\n",
    "\n",
    "print(set(df['outlook']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate probability of a feature given the class (play) we use conditinoal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.49999999750000007, 1: 0.49999999750000007, 2: 4.999999925000001e-09}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conditional_prob(df, feature, c, val):\n",
    " # c is the class (0 or 1)\n",
    " df2 = df[df[c] == val][feature]\n",
    " vals_counts = [[val, (df2 == val).sum() + 1e-8] for val in set(df[feature])]\n",
    " total_count = sum([count for val, count in vals_counts])\n",
    " \n",
    " vals_probs = [(val, count/total_count) for val, count in vals_counts]\n",
    " return dict(vals_probs)\n",
    "\n",
    "\n",
    "conditional_prob(df,'outlook','play',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use Bayes rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (1, 2)]\n",
      "[(0, 3), (1, 2)]\n",
      "probability of not playing: 0.0689655189536266\n",
      "probability of playing : 0.9310344810463733\n"
     ]
    }
   ],
   "source": [
    "o = 1\n",
    "t = 0\n",
    "h = 0\n",
    "w = 0\n",
    "\n",
    "c = 0\n",
    "p0 = marginal_prob(df, 'play')[c] * conditional_prob(df, 'outlook', 'play', c)[o] * conditional_prob(df, 'temp', 'play', c)[t] \\\n",
    "* conditional_prob(df, 'humid', 'play', c)[h] * conditional_prob(df, 'wind', 'play', c)[w]\n",
    "\n",
    "c = 1\n",
    "p1 = marginal_prob(df, 'play')[c] * conditional_prob(df, 'outlook', 'play', c)[o] * conditional_prob(df, 'temp', 'play', c)[t] \\\n",
    "* conditional_prob(df, 'humid', 'play', c)[h] * conditional_prob(df, 'wind', 'play', c)[w]\n",
    "\n",
    "# normalizing\n",
    "p_sum = p0 + p1\n",
    "p0 /= p_sum\n",
    "p1 /= p_sum\n",
    "\n",
    "print(\"probability of not playing: {}\".format(p0))\n",
    "print(\"probability of playing : {}\".format(p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Example 2\n",
    "\n",
    "Suppose we have documents below as our training set. \n",
    "\n",
    " d1: Chinese Beijing Chinese , class = C\n",
    " d2: Chinese Chinese Shanghai, class = C\n",
    " d3: Chinese Macao , class = C\n",
    " d4: Tokyo Japan Chinese , class = J\n",
    "\n",
    "\n",
    "**Exercise:** Train a NB classifier and predict if `d5` belongs to class C or J.\n",
    "\n",
    " d5: Chinese Chinese Tokyo Japan, class = ?\n",
    " \n",
    "What about `d6: Chinese Chinese Chinese Tokyo Japan, class = ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training documents\n",
    "docs = [\n",
    "    {'words': ['Chinese', 'Beijing', 'Chinese'], 'class': 'C'},\n",
    "    {'words': ['Chinese', 'Chinese', 'Shanghai'], 'class': 'C'},\n",
    "    {'words': ['Chinese', 'Macao'], 'class': 'C'},\n",
    "    {'words': ['Tokyo', 'Japan', 'Chinese'], 'class': 'J'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of documents in each class\n",
    "class_count = defaultdict(int)\n",
    "for doc in docs:\n",
    "    class_count[doc['class']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the prior probability of each class\n",
    "class_prob = {}\n",
    "total_count = len(docs)\n",
    "for c in class_count:\n",
    "    class_prob[c] = class_count[c] / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of occurrences of each word in each class\n",
    "word_count = defaultdict(lambda: defaultdict(int))\n",
    "for doc in docs:\n",
    "    c = doc['class']\n",
    "    for word in doc['words']:\n",
    "        word_count[word][c] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the conditional probability of each word given each class\n",
    "word_prob = defaultdict(lambda: defaultdict(float))\n",
    "alpha = 1  # Laplace smoothing parameter\n",
    "for word in word_count:\n",
    "    total_c = sum(word_count[word].values())\n",
    "    for c in class_count:\n",
    "        word_prob[word][c] = (word_count[word][c] + alpha) / (total_c + alpha * len(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5: Chinese Chinese Tokyo Japan, class = C\n"
     ]
    }
   ],
   "source": [
    "# Classify a new document\n",
    "new_doc = {'words': ['Chinese', 'Chinese', 'Tokyo', 'Japan']}\n",
    "doc_prob = defaultdict(float)\n",
    "for c in class_prob:\n",
    "    doc_prob[c] = math.log(class_prob[c])\n",
    "    for word in new_doc['words']:\n",
    "        if word in word_prob:\n",
    "            doc_prob[c] += math.log(word_prob[word][c])\n",
    "    # We can skip calculating P(d) because it's the same for all classes\n",
    "predicted_class = max(doc_prob, key=doc_prob.get)\n",
    "print(\"d5: Chinese Chinese Tokyo Japan, class = \" + predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d6: Chinese Chinese Chinese Tokyo Japan, class = C\n"
     ]
    }
   ],
   "source": [
    "new_doc = {'words': ['Chinese', 'Chinese', 'Chinese', 'Tokyo', 'Japan']}\n",
    "doc_prob = defaultdict(float)\n",
    "for c in class_prob:\n",
    "    doc_prob[c] = math.log(class_prob[c])\n",
    "    for word in new_doc['words']:\n",
    "        if word in word_prob:\n",
    "            doc_prob[c] += math.log(word_prob[word][c])\n",
    "predicted_class = max(doc_prob, key=doc_prob.get)\n",
    "print(\"d6: Chinese Chinese Chinese Tokyo Japan, class = \" + predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
